{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping with Beautiful Soup\n",
    "*****\n",
    "\n",
    "\n",
    "## Intro\n",
    "\n",
    "In this tutorial, we'll be scraping information on the state senators of Illinois, available [here](http://www.ilga.gov/senate), as well as the list of bills each senator has sponsored (e.g., [here](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tools\n",
    "\n",
    "1. [Requests](http://docs.python-requests.org/en/latest/user/quickstart/) - module used to get webpages \n",
    "2. [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) - module we use to parse and pull out part of the HTML returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using Beautiful Soup\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Make a Get Request and Read in HTML\n",
    "\n",
    "We use `requests` library to:\n",
    "1. make a GET request to the page http://www.ilga.gov/senate/default.asp\n",
    "2. read in the html of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "# read the content of the server’s response\n",
    "src = req.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the src object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Soup it\n",
    "\n",
    "Now we use the `BeautifulSoup` function to parse the reponse into an HTML tree. This returns an object (called a **soup object**) which contains all of the HTML in the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the response into an HTML tree\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "# take a look\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Find Elements\n",
    "\n",
    "BeautifulSoup has a number of functions to find things on a page. Like other webscraping tools, Beautiful Soup lets you find elements by their:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with an a particular HTML tag, and returns all of those elements.\n",
    "\n",
    "What does the example below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find all elements in a certain tag\n",
    "# these two lines of code are equivilant\n",
    "\n",
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: Because `find_all()` is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object as though it were a function, then it’s the same as calling `find_all()` on that object. \n",
    "\n",
    "These two lines of code are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all(\"a\")\n",
    "soup(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot! Many elements on a page will have the same html tag. For instance, if you search for everything with the `a` tag, you're likely to get a lot of stuff, much of which you don't want. What if we wanted to search for HTML tags ONLY with certain attributes, like particular CSS classes? \n",
    "\n",
    "We can do this by adding an additional argument to the `find_all`\n",
    "\n",
    "In the example below, we are finding all the `a` tags, and then filtering those with `class = \"sidemenu\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the 'a' tags in 'sidemenu' class\n",
    "soup(\"a\", class_=\"sidemenu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes a more efficient way to search and find things on a website is by **CSS selector.** For this we have to use a different method, `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
    "\n",
    "In the example above, we can use \"a.sidemenu\" as a CSS selector, which returns all `a` tags with class `sidemenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get elements with \"a.sidemenu\" CSS Selector.\n",
    "soup.select(\"a.sidemenu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Find all the `<a>` elements in class `mainmenu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Get Attributes and Text of Elements\n",
    "\n",
    "Once we identify elements, we want the access information in that element. Oftentimes this means two things:\n",
    "\n",
    "1. Text\n",
    "2. Attributes\n",
    "\n",
    "Getting the text inside an element is easy. All we have to do is use the `text` member of a `tag` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a list\n",
    "soup.select(\"a.sidemenu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first want to get an individual tag object\n",
    "first_link = soup.select(\"a.sidemenu\")[0] #zero is the first element in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out its class\n",
    "type(first_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a tag! Which means it has a `text` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want the value of certain attributes. This is particularly relevant for `a` tags, or links, where the `href` attribute tells us where the link goes.\n",
    "\n",
    "You can access a tag’s attributes by treating the tag like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Find all the `href` attributes (url) from the mainmenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "****\n",
    "\n",
    "Believe it or not, that's all you need to scrape a website. Let's apply these skills to scrape http://www.ilga.gov/senate/default.asp?GA=98\n",
    "\n",
    "**NB: we're just going to scrape the 98th general assembly.**\n",
    "\n",
    "Our goal is to scrape information on each senator, including their:\n",
    "    - name\n",
    "    - district\n",
    "    - party\n",
    "\n",
    "## 2.1 First, make the get request and soup it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# read the content of the server’s response\n",
    "src = req.text\n",
    "# soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Find the right elements and text.\n",
    "\n",
    "Now let's try to get a list of rows in that table. Remember that rows are identified by the `tr` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tr elements\n",
    "rows = soup.find_all(\"tr\")\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember, `find_all` gets all the elements with the `tr` tag. We can use smart CSS selectors to get only the rows we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "print(rows[2].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `select` method on anything. Let's say we want to find everything with the CSS selector `td.detail` in an item of the list we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select only those 'td' tags with class 'detail'\n",
    "row = rows[2]\n",
    "detailCells = row.select('td.detail')\n",
    "detailCells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we're interested in the actual **text** of a website, not its tags. Remember, to get the text of an HTML element, use the `text` member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the text in each of those cells\n",
    "rowData = [cell.text for cell in detailCells]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the beautifulsoup tools with our basic python skills to scrape an entire web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check em out\n",
    "print(rowData[0]) # Name\n",
    "print(rowData[3]) # district\n",
    "print(rowData[4]) # party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Loop it all together\n",
    "\n",
    "Let's use a for loop to get 'em all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "\n",
    "# read the content of the server’s response\n",
    "src = req.text\n",
    "\n",
    "# soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "# loop through all rows\n",
    "for row in rows:\n",
    "    # select only those 'td' tags with class 'detail'\n",
    "    detailCells = row.select('td.detail')\n",
    "    \n",
    "    # get rid of junk rows\n",
    "    if len(detailCells) is not 5: \n",
    "        continue\n",
    "        \n",
    "    # Keep only the text in each of those cells\n",
    "    rowData = [cell.text for cell in detailCells] #this is a list comprehension \n",
    "    \n",
    "    # Collect information\n",
    "    name = rowData[0]\n",
    "    district = int(rowData[3])\n",
    "    party = rowData[4]\n",
    "    \n",
    "    # Store in a tuple\n",
    "    tup = (name,district,party) #tuple is a python data type\n",
    "    \n",
    "    # Append to list\n",
    "    members.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challege 3: Get HREF element pointing to members' bills. \n",
    "\n",
    "The code above retrieves information on:  \n",
    "\n",
    "    - the senator's name\n",
    "    - their district number\n",
    "    - and their party\n",
    "\n",
    "We now want to retrieve the URL for each senator's list of bills. The format for the list of bills for a given senator is:\n",
    "\n",
    "http://www.ilga.gov/senate/SenatorBills.asp + ? + GA=98 + &MemberID=**_memberID_** + &Primary=True\n",
    "\n",
    "to get something like:\n",
    "\n",
    "http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True\n",
    "\n",
    "You should be able to see that, unfortunately, _memberID_ is not currently something pulled out in our scraping code.\n",
    "\n",
    "Your initial task is to modify the code above so that we also **retrieve the full URL which points to the corresponding page of primary-sponsored bills**, for each member, and return it along with their name, district, and party.\n",
    "\n",
    "Tips: \n",
    "\n",
    "* To do this, you will want to get the appropriate anchor element (`<a>`) in each legislator's row of the table. You can again use the `.select()` method on the `row` object in the loop to do this — similar to the command that finds all of the `td.detail` cells in the row. Remember that we only want the link to the legislator's bills, not the committees or the legislator's profile page.\n",
    "* The anchor elements' HTML will look like `<a href=\"/senate/Senator.asp/...\">Bills</a>`. The string in the `href` attribute contains the **relative** link we are after. You can access an attribute of a BeatifulSoup `Tag` object the same way you access a Python dictionary: `anchor['attributeName']`. (See the <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentation</a> for more details).\n",
    "* NOTE: There are a _lot_ of different ways to use BeautifulSoup to get things done; whatever you need to do to pull that HREF out is fine. Posting on the etherpad is recommended for discussing different strategies.\n",
    "\n",
    "I've started out the code for you. Fill it in where it says `#YOUR CODE HERE` (Save the path into an object called `full_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "\n",
    "# read the content of the server’s response\n",
    "src = req.text\n",
    "\n",
    "# soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "# loop through all rows\n",
    "for row in rows:\n",
    "    # select only those 'td' tags with class 'detail'\n",
    "    detailCells = row.select('td.detail')\n",
    "    \n",
    "    # get rid of junk rows\n",
    "    if len(detailCells) is not 5: \n",
    "        continue\n",
    "        \n",
    "    # Keep only the text in each of those cells\n",
    "    rowData = [cell.text for cell in detailCells]\n",
    "    \n",
    "    # Collect information\n",
    "    name = rowData[0]\n",
    "    district = int(rowData[3])\n",
    "    party = rowData[4]\n",
    "    \n",
    "    # YOUR CODE HERE.\n",
    "    \n",
    "    # Store in a tuple\n",
    "    tup = (name, district, party, full_path)\n",
    "    \n",
    "    # Append to list\n",
    "    members.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test \n",
    "\n",
    "# members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Make a function\n",
    "\n",
    "Turn the code above into a function that accepts a URL, scrapes the URL for its senators, and returns a list of tuples containing information about each senator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR FUNCTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test you3 code!\n",
    "\n",
    "# senateMembers = get_members('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# len(senateMembers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Scrape Bills\n",
    "****\n",
    "\n",
    "## 3.1 Writing a Scraper Function\n",
    "\n",
    "Now we want to scrape the webpages corresponding to bills sponsored by each bills.\n",
    "\n",
    "Write a function called `get_bills(url)` to parse a given Bills URL. This will involve:\n",
    "\n",
    "  - requesting the URL using the <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a> library\n",
    "  - using the features of the `BeautifulSoup` library to find all of the `<td>` elements with the class `billlist`\n",
    "  - return a _list_ of tuples, each with:\n",
    "      - description (2nd column)\n",
    "      - chamber (S or H) (3rd column)\n",
    "      - the last action (4th column)\n",
    "      - the last action date (5th column)\n",
    "      \n",
    "I've started the function for you. Fill in the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE THIS FUNCTION\n",
    "def get_bills(url):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src)\n",
    "    rows = soup.select('tr')\n",
    "    bills = []\n",
    "    for row in rows:\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "               \n",
    "        tup = (bill_id, description, champber, last_action, last_action_date)\n",
    "        bills.append(tup)\n",
    "    return(bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to test your code:\n",
    "# test_url = senateMembers[0][3]\n",
    "# get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get all the bills\n",
    "\n",
    "Finally, create a dictionary `bills_dict` which maps a district number (the key) onto a list_of_bills (the value) eminating from that district. You can do this by looping over all of the senate members in `members_dict` and calling `get_bills()` for each of their associated bill URLs.\n",
    "\n",
    "NOTE: please call the function `time.sleep(0.5)` for each iteration of the loop, so that we don't destroy the state's web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test\n",
    "# bills_dict[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
